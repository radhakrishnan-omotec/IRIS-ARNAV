# -*- coding: utf-8 -*-
"""stars Category Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/stars-category-classification-cc28c0d8-e49e-4f14-8093-05d9547e0f8a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241204/auto/storage/goog4_request%26X-Goog-Date%3D20241204T093902Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2a6446b9ab461efd3b8209d936ca94069dd55efadc5323d311ea330193d27f0b5559a7ef003217fe27b2d619f499258b54fb478b0fcb64ce23fff43f7e68835dac0dd6216c4001b2639f0f3ac1c06ed75266baef862546d740726d932cff841a75d74c324485b67175cbacef77ed677caccf50962403ba8701141d83760c378af6c23a756a322ac42490bf46aac4da972614fb014a9e42a2b31191fe8c3d92d6f1b162cc14e952ab7028ec73896a451f8a0d4189b1e3c25562d34401066e643ba310603b80a5716c812557620ddd6789ae295ec703755570ff2ba1e026442c5cc83a160955759d562809f5bcebb3f2017f97a412fe8d4c3be5bca25a96c79a80
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
deepu1109_star_dataset_path = kagglehub.dataset_download('deepu1109/star-dataset')

print('Data source import complete.')

import pandas as pd
import numpy as np

"""**Temperature (K):** The measure of the thermal energy at the surface of the star.

**Luminosity (L/Lo):** The total energy output of the star per second (power), compared to the Sun.

**Radius (R/Ro):** The distance from the center of the star to its surface, relative to the Sun.

**Absolute Magnitude (Mv):** A logarithmic measure of the star’s true brightness, independent of distance.

**Star Type:** The classification based on the star’s nuclear fusion process and life stage.

**Star Category:** A grouping of stars by size, mass, and energy output characteristics.

**Star Color:** The wavelength of light emitted by the star, related to its temperature

**Spectral Class:** A classification of stars by temperature and absorption lines in the star’s spectrum.
"""

df=pd.read_csv(r"/kaggle/input/star-dataset/6 class csv.csv")
df

df.info()

df['Star type'].value_counts()

df['Star category']=df['Star type'].apply(lambda x:'Brown Dwarf' if x==0 else 'Red Dwarf' if x==1
                                         else 'White Dwarf' if x==2 else 'Main Sequence' if x==3
                                         else 'Supergiant' if x==4 else 'Hypergiant')

"""**Data Visualization**"""

import seaborn as sns
import matplotlib.pyplot as plt

fig,axs=plt.subplots(1,3,figsize=(15,7))
sns.scatterplot(data=df,x='Luminosity(L/Lo)',y='Temperature (K)',hue='Star category',ax=axs[0])
sns.scatterplot(data=df,x='Radius(R/Ro)',y='Temperature (K)',hue='Star category',ax=axs[1])
sns.scatterplot(data=df,x='Absolute magnitude(Mv)',y='Temperature (K)',hue='Star category',ax=axs[2])
plt.tight_layout()
plt.show()

"""**Temparature vs Luminosity**

1. Brown Dwarfs and Red Dwarfs (blue dots and orange dots) ussually have low Luminosity as well as low Temperature.
2. White Dwarfs (green dots) usually follow a certain pattern where when there is a slight increase in Luminosity the Temperature increases. They have lower Luminosity but higher temperature.
3. Main Sequence stars (red dots) mostly follow a clear pattern, where higher temperatures correspond to higher luminosities.
4. Supergiants (purple) and Hypergiants (brown) are generally more luminous but show a wider spread in temperature.

**Temperature vs Radius**
1. Main Sequence stars again follow a pattern of increasing radius with increasing temperature.
2. Supergiants and Hypergiants have much larger radii but are spread across a wide temperature range.
3. White Dwarfs show small radii despite high temperatures, consistent with their classification.
4. Red and Brown Dwarfs have low temperatures and small radii, clustering towards the bottom left.

**Temperature vs Absolute Magnitude**
1. Main Sequence stars are well-distributed with a strong negative correlation: stars with higher temperatures tend to have lower (brighter) magnitudes.
2. Supergiants and Hypergiants have very bright magnitudes and a wide range of temperatures.
3. White Dwarfs have relatively high absolute magnitudes (dim) despite their high temperatures.
4. Red and Brown Dwarfs cluster at the bottom right, having low temperatures and high (dim) absolute magnitudes.*
"""

fig, axs = plt.subplots(1, 3, figsize=(18, 7))
sns.boxplot(data=df, x='Star category', y='Luminosity(L/Lo)', ax=axs[0])
sns.boxplot(data=df, x='Star category', y='Radius(R/Ro)', ax=axs[1])
sns.boxplot(data=df, x='Star category', y='Temperature (K)', ax=axs[2])
plt.tight_layout()
plt.show()

"""**Luminosity (L/Lo) vs Star Category (Left Plot)**

1. Brown Dwarfs, Red Dwarfs, and White Dwarfs: These stars have very low luminosity, with minimal variation. There are outliers visible, but overall they are clustered near zero.
2. Main Sequence stars: Show a moderate increase in luminosity, but still much lower than Supergiants and Hypergiants. The variation is much larger compared to dwarfs.
3. Supergiants and Hypergiants: Have significantly higher luminosity values, with some extreme outliers reaching even greater luminosities. Hypergiants in particular show a very wide range in luminosity.

**Radius (R/Ro) vs Star Category (Middle Plot)**
1. Brown Dwarfs, Red Dwarfs, White Dwarfs: All show very small radii, clustering near the bottom, similar to their small luminosity.
2. Main Sequence stars: Show a slight increase in radius, but remain significantly smaller than Supergiants and Hypergiants.
3. Supergiants and especially Hypergiants: Have enormous radii, with Hypergiants showing much more spread, indicating that Hypergiants can vary widely in size.

**Temperature (K) vs Star Category (Right Plot)**
1. Brown Dwarfs, Red Dwarfs, and White Dwarfs: These stars have low temperatures, with White Dwarfs being the hottest among these categories. The temperature spread is relatively narrow.
2. Main Sequence stars: Show a broader range of temperatures, with some stars reaching very high temperatures compared to dwarfs.
3. Supergiants and Hypergiants: These stars exhibit a wide temperature range, with Supergiants generally showing more variability. Some can be as hot as Main Sequence stars, while others are much cooler.

**The above scatter and box plots provide us with these General Insights**
1. Dwarf stars (Brown, Red, and White) tend to have low luminosity, small radii, and low-to-moderate temperatures.
2. Main Sequence stars are more moderate in all characteristics, but show clear patterns of increasing luminosity, radius, and temperature compared to dwarfs.
3. Supergiants and Hypergiants are the largest and brightest stars, with significant variation in size and temperature. They have a much higher spread
in luminosity, massive radii, and varying temperatures.
"""

plt.figure(figsize=(15,8))
sns.scatterplot(data=df, x='Radius(R/Ro)', y='Luminosity(L/Lo)', hue='Star category')
plt.show()

plt.figure(figsize=(13,8))
sns.boxplot(data=df, x='Star category', y='Absolute magnitude(Mv)')
plt.show()

"""**Brown Dwarfs:** These stars have the highest absolute magnitude, ranging between 15 and 20, making them the dimmest category in the chart.

**Red Dwarfs:** Slightly brighter than Brown Dwarfs, Red Dwarfs have an absolute magnitude range of approximately 10 to 15.

**White Dwarfs:** These stars are a bit brighter than Red Dwarfs, with absolute magnitudes clustering around 10 to 13.

**Main Sequence Stars:** These stars have a wide range of absolute magnitudes, from around -5 to +10, indicating a broader variation in brightness.

**Supergiants:** They are very bright stars, with magnitudes ranging from -5 to about -7. They are more luminous than Main Sequence stars and much more luminous than dwarf stars.

**Hypergiants:** These stars are the brightest category on the chart, with absolute magnitudes ranging from approximately -8 to -10, showing extreme brightness.
"""

sns.scatterplot(data=df, x=(df['Luminosity(L/Lo)'] ** (1/3.5)), y='Luminosity(L/Lo)', hue='Star category')
plt.xlabel('Mass(M/Mo)')
plt.show()

"""**The above plot describing the relationship between Mass (derived from Luminosity) and Luminosity tell us the follwing things:**

1. The plot demonstrates that stars with larger masses emit significantly more energy, with hypergiants and supergiants showing the highest values for
both mass and luminosity.
2. Brown dwarfs, red dwarfs, and white dwarfs are far less luminous and less massive, clustered towards the bottom left.
3. This exponential increase in luminosity with mass is a characteristic of stellar evolution, where massive stars burn their fuel more rapidly and shine more brightly.

**Data Preprocessing and Feature Engineering**
"""

df.info()

for col in df.columns[df.dtypes=='object']:
    print(df[col].value_counts(),'\n\n\n')

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()
for col in df.columns[df.dtypes=='object']:
    df[col]=le.fit_transform(df[col])

df['Mass (M/Mo)']=(df['Luminosity(L/Lo)']**(1/3.5))

from sklearn.feature_selection import mutual_info_classif
t_x=df.drop(columns='Star category')
t_y=df['Star category']
mi=mutual_info_classif(t_x,t_y)
mi_scores_df=pd.DataFrame({'Feature': t_x.columns, 'MI Score':mi})
mi_scores_df=mi_scores_df.sort_values(by='MI Score', ascending=False)
print(mi_scores_df)

df['Magnitude_Temperature_Ratio']=df['Absolute magnitude(Mv)']/df['Temperature (K)']

mi=mutual_info_classif(df[['Magnitude_Temperature_Ratio']],t_y)
mi_scores_df=pd.DataFrame({'Feature':['Magnitude_Temperature_Ratio'], 'MI Score':mi})
mi_scores_df=mi_scores_df.sort_values(by='MI Score', ascending=False)
print(mi_scores_df)

df['Radius_Luminosity_Ratio']=(df['Radius(R/Ro)']/df['Luminosity(L/Lo)'])

mi=mutual_info_classif(df[['Radius_Luminosity_Ratio']],t_y)
mi_scores_df=pd.DataFrame({'Feature':['Radius_Luminosity_Ratio'], 'MI Score':mi})
mi_scores_df=mi_scores_df.sort_values(by='MI Score', ascending=False)
print(mi_scores_df)

t_x=df.drop(columns='Star category')
t_y=df['Star category']
mi=mutual_info_classif(t_x,t_y)
mi_scores_df=pd.DataFrame({'Feature': t_x.columns, 'MI Score':mi})
mi_scores_df=mi_scores_df.sort_values(by='MI Score', ascending=False)
print(mi_scores_df)

df.info()

"""**Data Preparation**"""

from sklearn.model_selection import train_test_split

x=df.drop(columns='Star category')
y=df['Star category']
x_t,x_te,y_t,y_te=train_test_split(x,y,test_size=0.25,random_state=20)

from sklearn.model_selection import RandomizedSearchCV,GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,precision_score,f1_score,recall_score

"""**AdaBoost Classifier**"""

ada=AdaBoostClassifier(algorithm='SAMME')
ada_xt=x_t.copy()
ada_xte=x_te.copy()
ada_yt=y_t.copy()
ada_yte=y_te.copy()
param={'estimator': [DecisionTreeClassifier(max_depth=1),None],
    'learning_rate': np.arange(0.1, 2.01, 0.01),
    'n_estimators': [300]}
nada=RandomizedSearchCV(ada,param_distributions=param,cv=10,n_jobs=-1,random_state=20,scoring='accuracy')
nada.fit(ada_xt,ada_yt)
print(nada.best_params_)
print(nada.best_score_)

nada=nada.best_estimator_
pred1_t=nada.predict(ada_xt)
pred1=nada.predict(ada_xte)

"""**Decision Tree Classifier**"""

dt_xt=x_t.copy()
dt_xte=x_te.copy()
dt_yt=y_t.copy()
dt_yte=y_te.copy()
dt=DecisionTreeClassifier()
path=dt.cost_complexity_pruning_path(dt_xt,dt_yt)
alphas=path.ccp_alphas
param={'ccp_alpha':alphas}
ndt = GridSearchCV(dt, param_grid=param, n_jobs=-1, scoring='accuracy')
ndt.fit(dt_xt,dt_yt)
print(ndt.best_params_)
print(ndt.best_score_)
ba=ndt.best_params_['ccp_alpha']

dt=DecisionTreeClassifier(ccp_alpha=ba)
param={'criterion':['gini','entropy'],'min_samples_split':list(np.arange(2,51)),'min_samples_leaf':list(np.arange(1,51)),
       'max_features':['sqrt','log2',None]}
ndt=RandomizedSearchCV(dt,param_distributions=param,cv=10,n_jobs=-1,random_state=20,scoring='accuracy')
ndt.fit(dt_xt,dt_yt)
print(ndt.best_params_)
print(ndt.best_score_)
ndt=ndt.best_estimator_

pred2_t=ndt.predict(dt_xt)
pred2=ndt.predict(dt_xte)

"""**Random Forest Classifier**"""

rf_xt=x_t.copy()
rf_xte=x_te.copy()
rf_yt=y_t.copy()
rf_yte=y_te.copy()
rf=RandomForestClassifier()
param={'criterion':['gini','entropy'],'min_samples_split':list(np.arange(2,52)),
       'min_samples_leaf':list(np.arange(1,52)),
       'n_estimators':[400]}
nrf=RandomizedSearchCV(rf,param_distributions=param,n_jobs=-1,random_state=20,cv=10)
nrf.fit(rf_xt,rf_yt)
print(nrf.best_params_)
print(nrf.best_score_)

nrf=nrf.best_estimator_
pred3_t=nrf.predict(rf_xt)
pred3=nrf.predict(rf_xte)

tdf=pd.DataFrame({
    'Classification Algorithms': ['AdaBoost Classifier', 'Decision Tree Classifier', 'Random Forest Classifier'],
    'Training Accuracy': [
        accuracy_score(ada_yt, pred1_t),
        accuracy_score(dt_yt, pred2_t),
        accuracy_score(rf_yt, pred3_t)
    ],
    'Training Precision': [
        precision_score(ada_yt, pred1_t, average='weighted', zero_division=0),
        precision_score(dt_yt, pred2_t, average='weighted', zero_division=0),
        precision_score(rf_yt, pred3_t, average='weighted', zero_division=0)
    ],
    'Training Recall': [
        recall_score(ada_yt, pred1_t, average='weighted'),
        recall_score(dt_yt, pred2_t, average='weighted'),
        recall_score(rf_yt, pred3_t, average='weighted')
    ],
    'Training F1 Score': [
        f1_score(ada_yt, pred1_t, average='weighted'),
        f1_score(dt_yt, pred2_t, average='weighted'),
        f1_score(rf_yt, pred3_t, average='weighted')
    ]
})
tdf

tedf=pd.DataFrame({
    'Classification Algorithms': ['AdaBoost Classifier', 'Decision Tree Classifier', 'Random Forest Classifier'],
    'Testing Accuracy': [
        accuracy_score(ada_yte, pred1),
        accuracy_score(dt_yte, pred2),
        accuracy_score(rf_yte, pred3)
    ],
    'Testing Precision': [
        precision_score(ada_yte, pred1, average='weighted', zero_division=0),
        precision_score(dt_yte, pred2, average='weighted', zero_division=0),
        precision_score(rf_yte, pred3, average='weighted', zero_division=0)
    ],
    'Testing Recall': [
        recall_score(ada_yte, pred1, average='weighted'),
        recall_score(dt_yte, pred2, average='weighted'),
        recall_score(rf_yte, pred3, average='weighted')
    ],
    'Testing F1 Score': [
        f1_score(ada_yte, pred1, average='weighted'),
        f1_score(dt_yte, pred2, average='weighted'),
        f1_score(rf_yte, pred3, average='weighted')
    ]
})
tedf

print('AdaBoost Classifier classification report\n\n',classification_report(ada_yte,pred1,zero_division=0),'\n\n\n')
print('Decision Tree Classifier classification report\n\n',classification_report(dt_yte,pred2,zero_division=0),'\n\n\n')
print('Random Forest Classifier classification report\n\n',classification_report(dt_yte,pred3,zero_division=0))

fig,axs=plt.subplots(1,3,figsize=(15,6))
fig.suptitle('Training Confusion Matrices for different Classification Models', fontsize=16)
sns.heatmap(confusion_matrix(ada_yt,pred1_t),annot=True,ax=axs[0])
axs[0].set_title('AdaBoost Classifier')
sns.heatmap(confusion_matrix(dt_yt,pred2_t),annot=True,ax=axs[1])
axs[1].set_title('Decision Tree Classifier')
sns.heatmap(confusion_matrix(dt_yt,pred3_t),annot=True,ax=axs[2])
axs[2].set_title('Random Forest CLassifier')
plt.tight_layout()
plt.show()

fig,axs=plt.subplots(1,3,figsize=(15,6))
fig.suptitle('Testing Confusion Matrices for different Classification Models', fontsize=16)
sns.heatmap(confusion_matrix(ada_yte,pred1),annot=True,ax=axs[0])
axs[0].set_title('AdaBoost Classifier')
sns.heatmap(confusion_matrix(dt_yte,pred2),annot=True,ax=axs[1])
axs[1].set_title('Decision Tree Classifier')
sns.heatmap(confusion_matrix(dt_yte,pred3),annot=True,ax=axs[2])
axs[2].set_title('Random Forest CLassifier')
plt.tight_layout()
plt.show()

"""**AdaBoost Classifier:**

1. Training Performance: It shows decent accuracy (83.89%) and reasonable precision (75.44%), recall (83.89%), and F1 score (78.35%).
2. Testing Performance: The testing accuracy (81.67%) is slightly lower than the training accuracy, indicating potential overfitting, though the metrics remain relatively stable.
****


**Decision Tree Classifier:**

1. Training Performance: It achieved perfect scores across all metrics (accuracy, precision, recall, F1 score) on the training data (100%).
2. Testing Performance: Similarly, it also scored perfectly on the testing data. This suggests that the model may be overfitting the data, as it performs exceptionally well on both training and testing.
****


**Random Forest Classifier:**

1. Training Performance: Like the Decision Tree, it also achieved perfect scores on the training data.
2. Testing Performance: The testing accuracy (98.33%) is very high, with precision (98.54%), recall (98.33%), and F1 score (98.33%) indicating strong generalization capability. This suggests it balances well between fitting the training data and performing well on unseen data.
****


**Overall Conclusions:**

1. The Decision Tree and Random Forest Classifiers exhibit perfect performance on both training and testing sets, indicating they may be overfitting the training data, but the Random Forest shows better generalization compared to the Decision Tree.
2. The AdaBoost Classifier performs adequately but does not reach the high performance of the other two models, suggesting it may be a less optimal choice for this particular dataset.
3. When choosing a model, the Random Forest might be the best option due to its strong testing performance, while the Decision Tree might need further tuning to avoid overfitting.

"""